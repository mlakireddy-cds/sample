{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M0_MP1_Data_Munging_(Ungraded)_Group2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlakireddy-cds/sample/blob/main/M0_MP1_Data_Munging_(Ungraded)_Group2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUu9l_JfJ92"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook 1 : Data munging\n",
        "\n",
        "(Ungraded Mini-Project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL3yrUc-XrLS"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "\n",
        "\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "\n",
        "* understand the requirements for a “clean” dataset, ready for use in statistical analysis.\n",
        "\n",
        "* use Python libraries like Pandas, Numpy, and Matplotlib to perform the  data-preprocessing steps accordingly.\n",
        "\n",
        "* derive meaningful insights from the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EqvTSjZZIUE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZX_NxQHZLu_"
      },
      "source": [
        "The dataset chosen for this experiment is **play store** dataset which is  publicly available and created with this [methodology](https://nycdatascience.com/blog/student-works/google-play-store-everything-that-you-need-to-know-about-the-android-market/)  \n",
        "\n",
        "This dataset consists of 10841 records. Each record is made up of 13 fields.\n",
        "\n",
        "**For example**, one record consist of App, Category, Rating, Reviews, Size, Installs, Type, Price, Content Rating, Genres, Last Updated, Current Ver, and Android Ver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vgcWwOF2cK"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VTtVENRXo7i"
      },
      "source": [
        "Before we can derive any meaningful insights from the Play Store data, it is essential to pre-process the data and make it suitable for further analysis. This pre-processing step forms a major part of data wrangling (or data munging) and ensures better quality data. It consists of the transformation and mapping of data from a \"raw\" data form into another format so that it is more valuable for a variety of downstream purposes such as analytics. Data analysts typically spend a sizeable amount of time in the process of data wrangling, compared to the actual analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CgkmzMOXo7j"
      },
      "source": [
        "After data munging is performed, several actionable insights can be derived from the Play Store apps data. Such insights could help to unlock the enormous potential to drive app-making businesses to success."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qwmmvQnv3mLM"
      },
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/googleplaystore.csv\n",
        "print(\"Data downloaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmOJDVdp9PYo"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrVCIg54LZp"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"googleplaystore.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li5KS0i3pQqq"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP11Ov2mXo7p"
      },
      "source": [
        "There are different steps involved in Data Preprocessing. These steps are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NwZ6jBHXo7q"
      },
      "source": [
        "    1. Data Cleaning → In this step the primary focus is on\n",
        "        -Handling missing data\n",
        "        -Handling noisy data\n",
        "        -Detection and removal of outliers\n",
        "    \n",
        "    2. Data Integration → This process is used when data is gathered from various data sources\n",
        "    and data are combined to form consistent data. This data after performing cleaning is used\n",
        "    for analysis.\n",
        "    \n",
        "    3. Data Transformation → In this step we will convert the raw data into a specified format\n",
        "    according to the need of the model we are building. There are many options used for\n",
        "    transforming the data as below:\n",
        "        -Normalization\n",
        "        -Aggregation\n",
        "        -Generalization\n",
        "        \n",
        "    4. Data Reduction → After data transformation and scaling the redundancy within the data\n",
        "    is removed and efficiently organizing the data is performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edt4IHsO4lua"
      },
      "source": [
        "### Task 1: Data Cleaning\n",
        "\n",
        "* Check whether there are any null values and figure out how you want to handle them? \n",
        "  \n",
        "    **Hint:** isnan(), dropna(), fillna()\n",
        "* If there is any duplication of a record, how would you like to handle it?\n",
        "\n",
        "    Hint: [drop_duplicates](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)\n",
        "\n",
        "* Are there any non-English apps? And how to filter them?\n",
        "\n",
        "* In the size column, multiply 10,000,000 with M in the cell and multiply by 10,000 if we have K in the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgCf41v3FHLw"
      },
      "source": [
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1csj1dxBtOv"
      },
      "source": [
        "def CategoryColCleaning(s: str) -> str:\n",
        "    if bool(re.search(r'^[A-Z_]+$',s)):\n",
        "        return s\n",
        "    else:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_p6mVgFBtOv"
      },
      "source": [
        "def RatingColCleaning(f: float) -> float:\n",
        "    if (f<=5.0) and (f>=0.0):\n",
        "        return f\n",
        "    else:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCSM34UjBtOw"
      },
      "source": [
        "def ReviewColCleaning(s: str) -> str:\n",
        "    if bool(re.search(r'^[0-9]+$',s)):\n",
        "        return int(s)\n",
        "    else:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6f3XocBtOw"
      },
      "source": [
        "conversion={'M': 1000000, 'K': 1000}\n",
        "def SizeColCleaning(s: str) -> int:\n",
        "    a,b=s[:-1],s[-1]\n",
        "    if (bool(re.search(r'^[0-9.]+$',a))) and (b.upper() in conversion.keys()):\n",
        "        return int(float(a)*conversion[b.upper()])\n",
        "    else:\n",
        "        return pd.NA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dplHAVt2BtOw"
      },
      "source": [
        "def InstallsColCleaning(s: str) -> int:\n",
        "    s=''.join((s.split(',')))\n",
        "    a,b=s[:-1],s[-1]\n",
        "    if bool(re.search(r'^[0-9]+$',s)):\n",
        "        return int(s)\n",
        "    elif (bool(re.search(r'^[0-9]+$',a))) and (b=='+'):\n",
        "        return int(a)\n",
        "    else:\n",
        "        return pd.NA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kayq8D5kBtOw"
      },
      "source": [
        "def TypeColCleaning(s: str) -> str:\n",
        "    if s in ['Free','Paid']:\n",
        "        return s\n",
        "    else:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvpRQ3ndBtOx"
      },
      "source": [
        "def PriceColCleaning(s: str) -> int:\n",
        "    a,b=s[0],s[1:]\n",
        "    if bool(re.search(r'^\\s*[0-9]*[.]*[0-9]+\\s*$',s)):\n",
        "        return float(s)\n",
        "    elif (a=='$') and (bool(re.search(r'^\\s*[0-9]*[.]*[0-9]+\\s*$',b))):\n",
        "        return float(b)\n",
        "    else:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-Jpo0PBtOx"
      },
      "source": [
        "df[\"Category\"]=df[\"Category\"].map(CategoryColCleaning)\n",
        "df[\"Rating\"]=df[\"Rating\"].map(RatingColCleaning)\n",
        "df[\"Reviews\"]=df[\"Reviews\"].map(ReviewColCleaning)\n",
        "df[\"Size\"]=df[\"Size\"].map(SizeColCleaning)\n",
        "df[\"Installs\"]=df[\"Installs\"].map(InstallsColCleaning)\n",
        "df[\"Type\"]=df[\"Type\"].map(TypeColCleaning)\n",
        "df[\"Price\"]=df[\"Price\"].map(PriceColCleaning)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df=df[~((df[\"Category\"].isna())\\\n",
        "   &(df[\"Rating\"].isna())\\\n",
        "   &(df[\"Reviews\"].isna())\\\n",
        "   &(df[\"Size\"].isna())\\\n",
        "   &(df[\"Installs\"].isna())\\\n",
        "   &(df[\"Type\"].isna())\\\n",
        "   &(df[\"Price\"].isna()))]\n",
        "\n",
        "print(df.shape)\n",
        "df=df[(df[\"Type\"].notna())]\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(df.shape)\n",
        "# rating and size have nulls."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPzlHc4-zIR"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFgtC_jCpJL1"
      },
      "source": [
        "### Task 2: Perform the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ksl14_hBtOy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.close(\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QQ2WUQX9XYy"
      },
      "source": [
        "##### Exercise 1: Find the number of apps in various categories by using an appropriate plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcRjzr9YRo72"
      },
      "source": [
        "# s21=df.groupby(\"Category\")[\"Category\"].agg(np.size)\n",
        "df.Category.value_counts().plot(kind='barh', color=['red', 'green', 'blue', 'cyan','black'], figsize = (20,15))\n",
        "# df.Category.value_counts().head(10)\n",
        "# s21.sort_values()\n",
        "# s21.plot.bar(rot=90)\n",
        "# Pichart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzzEnMv25vGn"
      },
      "source": [
        "##### Exercise 2: Explore the distribution of free and paid apps across different categories\n",
        "\n",
        "**Hint:** Stacked Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzxR7Gj4Rrbw"
      },
      "source": [
        "s22=df.groupby([\"Category\",\"Type\"]).size()\n",
        "print(s22)\n",
        "df22_Unstacked=s22.unstack(1)\n",
        "print(df22_Unstacked)\n",
        "# df22_Unstacked\n",
        "df22_Unstacked.plot.bar(rot=90, stacked=True)\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS0NM2wXBtOz"
      },
      "source": [
        "df.groupby(['Category','Type']).size().unstack().plot(kind=\"barh\", stacked=True, figsize=(20,20), width=0.5)\n",
        "plt.title(\"Distribution of free and paid apps across different categories\")\n",
        "plt.xlabel(\"Type Distribution\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNIQ4dj59Ep"
      },
      "source": [
        "##### Exercise 3: Represent the distribution of app rating on a scale of 1-5 using an appropriate plot\n",
        "\n",
        "**Hint:** histogram / strip plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDWxb_JRtBl"
      },
      "source": [
        "s23=df[(df[\"Rating\"].notna())][\"Rating\"]\n",
        "s23.plot.hist(bins=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBreBsQ7Vqi0"
      },
      "source": [
        "##### Exercise 4: Identify outliers of the rating column by plotting the boxplot category wise and Handle them.\n",
        "\n",
        "**Hint:** Removing Outliers using z-score, quantile [link](https://kanoki.org/2020/04/23/how-to-remove-outliers-in-python/) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehVoOe9ARv0w"
      },
      "source": [
        "df24=df[(df[\"Rating\"].notna())][[\"App\",\"Rating\"]]\n",
        "\n",
        "df24.plot.box()\n",
        "\n",
        "df24[\"Z_Score\"]=(df24[\"Rating\"] - df24[\"Rating\"].mean()) / df24[\"Rating\"].std()\n",
        "\n",
        "df24_Outliers=df24[df24['Z_Score'].abs()>3]\n",
        "print(df24_Outliers)\n",
        "\n",
        "# suggestion --> replacing feature valuse with Q1 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiOWt855DsZ8"
      },
      "source": [
        "##### Exercise 5: Plot the barplot of all the categories indicating no. of installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LW5CejRyBc"
      },
      "source": [
        "s25=df.groupby(\"Category\").agg(Installs=(\"Installs\", \"sum\"))\n",
        "s25.plot.bar(rot=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLNboJI1bDhL"
      },
      "source": [
        "## Insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBhK2SdGXlW"
      },
      "source": [
        "### Task 3: Derive the below insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVLkGB_ANwf"
      },
      "source": [
        "##### Exercise 1: Does the price correlate with the size of the app?\n",
        "\n",
        "  **Hint:** plot the scatterplot of `Size` and `Price`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDhPtmBCJC41"
      },
      "source": [
        "df31=df[(df[\"Size\"].notna()) & (df[\"Type\"]=='Paid')][[\"Size\",\"Price\"]]\n",
        "df31.plot.scatter(x=\"Size\", y=\"Price\")\n",
        "# No Correlation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb861WejndKI"
      },
      "source": [
        "##### Exercise 2: Find the popular app categories based on rating and no. of installs\n",
        "\n",
        "**Hint:** [df.groupby.agg()](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html); Taking the average rating could be another approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWftl4eC2jNU"
      },
      "source": [
        "df32=df[(df[\"Rating\"].notna())]\n",
        "df32=df32.groupby(\"Category\").agg(Rating_Mean=(\"Rating\", \"mean\"), Installs=(\"Installs\", \"sum\"))\n",
        "# df32=df32.sort_values(by=[\"Installs\", \"Rating_Mean\"])\n",
        "s32_Installs=df32[\"Installs\"].nlargest(3).index\n",
        "s32_Rating=df32[\"Rating_Mean\"].nlargest(3).index\n",
        "dict={\"Installs\" : list(s32_Installs),\n",
        "     \"Rating\" : list(s32_Rating)}\n",
        "dict\n",
        "\n",
        "# sum of rating can also be considered apart from mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kksy2sD4CMKQ"
      },
      "source": [
        "##### Exercise 3: How many apps are produced in each year category-wise ?\n",
        "\n",
        "  * Create a `Year` column by slicing the values of `Last Updated` column and find the Year with most no. of apps produced \n",
        "\n",
        "    **For example**, slice the year `2017` from `February 8, 2017` \n",
        "\n",
        "  * Find the categories which have a consistent rating in each year\n",
        "\n",
        "      **Hint:** `sns.countplot`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpnlYfHCO3P"
      },
      "source": [
        "def YearFromDate(s: str) -> int:\n",
        "    if bool(re.search(r'^[0-9]{1,2}-[A-Za-z]{3}-[0-9]{1,2}$',s)):\n",
        "        return int(s.split('-')[2])\n",
        "    else:\n",
        "        return pd.NA\n",
        "\n",
        "df[\"Year\"]=df[\"Last Updated\"].map(YearFromDate)\n",
        "s33=df.groupby(\"Year\").size()\n",
        "print(s33.nlargest(1))\n",
        "\n",
        "df['Year'] = pd.DatetimeIndex(df['Last Updated']).year\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnhRhfWadnGK"
      },
      "source": [
        "##### Exercise 4: Identify the highest paid apps with a good rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LmV1w_JdvRg"
      },
      "source": [
        "df34=df[(df[\"Rating\"].notna()) & (df[\"Type\"]=='Paid')][[\"Rating\",\"Price\"]]\n",
        "\n",
        "df34[\"Rating_Std\"]=((df34[\"Rating\"]-df34[\"Rating\"].min())/(df34[\"Rating\"].max()-df34[\"Rating\"].min()))\n",
        "df34[\"Price_Std\"]=((df34[\"Price\"]-df34[\"Price\"].min())/(df34[\"Price\"].max()-df34[\"Price\"].min()))\n",
        "\n",
        "df34[\"R*P\"]=df34[\"Rating_Std\"]*df34[\"Price_Std\"]\n",
        "print(df34[df34[\"R*P\"]==df34[\"R*P\"].max()])\n",
        "\n",
        "df34.plot.scatter(x=\"Price\", y=\"Rating\")\n",
        "df34.plot.scatter(x=\"Price_Std\", y=\"Rating_Std\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCEb0GX5-d1"
      },
      "source": [
        "##### Exercise 5: Are the top-rated apps genuine ? How about checking reviews count of top-rated apps ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8kGpmMmx7HI"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQfqIixzC4_"
      },
      "source": [
        "##### Exercise 6: If the number of reviews of an app is very low, what could be the reason for its top-rating ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_gj_f-UzGEF"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}