{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "M1_AST_05_Bayesian_Inference_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlakireddy-cds/sample/blob/main/M1_AST_05_Bayesian_Inference_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmknFOMIWWa7"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 5: Bayesian Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWFwF2G4WWbD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6i50W1HWWbH"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand the terms like maximum likelihood estimates, priors, conjugate priors\n",
        "* apply the concept of Bayesian inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G89Jvc-VWWbI"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdAbsCMrWWbK"
      },
      "source": [
        "**Why we need Bayesian inference?**\n",
        "\n",
        "In general, statistical inference is the process of determining properties of a model/distribution, given some data. Bayesian inference can be seen as the Bayesian counterpart to frequentist inference. In Frequentist inference, there is usually the notion of some true, unknown, parameter which is a constant, and point estimates are inferred from data. Contrarily, Bayesian inference treats the model parameters as random variables and usually wants to deduce probabilistic statements about the distribution of parameters.\n",
        "\n",
        "**Terminology**\n",
        "\n",
        "The basic terms related to Bayesian inference are as follows:\n",
        "\n",
        "- **Prior:** the probability distribution that would express one's beliefs about an uncertain quantity before some evidence is taken into account.\n",
        "- **Posterior:**  in Bayesian statistics, it is the revised or updated probability of an event occurring after taking into consideration new information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN10wgdHwxWw"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojPtXEtYwxXV"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M1_AST_05_Bayesian_Inference_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C05eB0QfWWbN"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oElkahyIWWbO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import norm, beta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE6HA2LLWWbP"
      },
      "source": [
        "### Bayesian Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3glvKBVeWWbQ"
      },
      "source": [
        "Bayesian inference utilizes the famous Bayes rule:\n",
        "\n",
        "$$ P(A|B) = \\frac{P(B|A).P(A)}{P(B)} $$\n",
        "\n",
        "For model based inference, we can replace  $A$  with the parameters  $θ$  and  $B$  with the data  $D$  at interest. Furthermore, we can introduce  $I$  which can be used to introduce an additional assumption (knowledge) to the inference such as which model to use.\n",
        "\n",
        "$$ \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}} $$\n",
        "\n",
        "The prior distribution  $P(θ|I)$ specifies our assumption about the parameters  $θ$  before taking the data into account. The likelihood  $P(D|θ,I)$  represents the probability of the data if the parameters  $θ$  are specified. The marginal likelihood (or evidence)  $P(D|I)$  is the distribution of the data  $D$  given our additional assumption  $I$ . It is the normalization of the Bayes rule and plays an important role in model comparison. Finally, the posterior  $P(θ|D,I)$  is the distribution of the parameters after taking the observed data  $D$  and our additional (prior) assumption  $I$  into account. We can also say that the posterior is proportional to the likelihood and the prior.\n",
        "\n",
        "$$ posterior ∝ likelihood × prior $$\n",
        "\n",
        "To understand Bayesian inference interactively, click [here](https://seeing-theory.brown.edu/bayesian-inference/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGbLBdJkWWbw"
      },
      "source": [
        "**Exercise 1:** We believe the probability of getting heads in a coin toss to be some true value p, but have no prior opinion on what p is. So we flip a coin a few times and record what's observed for each flip. Let's see how do our beliefs change as we observe more data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YIP8dL2WWbx"
      },
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 100, 200, 500]    # specify different number of trials\n",
        "data = stats.bernoulli.rvs(0.5, size=n_trials[-1])         # store the outcome of 500 coin tosses\n",
        "x = np.linspace(0, 1, 100)                                 # possible values for probability\n",
        "\n",
        "for k, N in enumerate(n_trials):\n",
        "    sx = plt.subplot(len(n_trials)/2, 2, k+1)\n",
        "    plt.xlabel('$p$, probability of heads') if k in [0, len(n_trials)-1] else None\n",
        "    plt.setp(sx.get_yticklabels(), visible=False)\n",
        "    heads = data[:N].sum()                # get number of heads from N trials\n",
        "    a = 1                                 # alpha parameter for Beta distribution as prior\n",
        "    b = 1                                 # beta parameter for Beta distribution as prior\n",
        "    s = heads                             # number of successes observed in N trials\n",
        "    f = N-heads                           # number of failures observed in N trials\n",
        "    y = beta.pdf(x, a+s, b+f)             # posterior as Beta distribution with alpha= a+s, beta= b+f\n",
        "    plt.plot(x, y, label='observe %d tosses,\\n %d heads' % (N, heads))         # visualize posterior for N trials\n",
        "    plt.fill_between(x, 0, y, color='#348ABD', alpha=0.4)\n",
        "    plt.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
        "\n",
        "    leg = plt.legend()\n",
        "    leg.get_frame().set_alpha(0.4)\n",
        "    plt.autoscale(tight=True)\n",
        "\n",
        "plt.suptitle(\"Bayesian updating of posterior probabilities\",\n",
        "             y=1.02,\n",
        "             fontsize=14)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZzksPcnWWb0"
      },
      "source": [
        "From the above plots following observations can be made:\n",
        "\n",
        "* belief under no information is that all possible heads probabilities are equally likely (as in the case of 0 tosses above)\n",
        "* it may not be possible for a small number of observations to represent the true probability (as in the case of 3 tosses above)\n",
        "* small sample sizes are incredibly sensitive to minor imbalances in outcome counts\n",
        "* as the sample size grows, the pdf gets narrow and more probability is allocated to  p=0.5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axNZ1mbMWWb3"
      },
      "source": [
        "### Prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMcfm5wlWWb4"
      },
      "source": [
        "In Bayesian statistical inference, a prior probability distribution, often simply called the *prior*, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9FBqToLWWb6"
      },
      "source": [
        "**Exercise 2:** We are observing whether a coin flip results in \"heads\" or \"tails\". We are not sure whether the coin at interest is fair or whether it might be biased due to some asperity or similar things. Thus, we want to conduct statistical inference of the parameter  p, which should describe the probability of flipping \"heads\", by utilizing the Bayesian framework. The probability of flipping \"tails\" is simply  1−p. Further, we consider a set of observations  D  by flipping the coin several times. Thus, by applying Bayes inference, we want to determine:\n",
        "\n",
        "$$ P(p|D) \\propto P(D|p) \\times P(p) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3DCROtEWWb8"
      },
      "source": [
        "**Model:**\n",
        "\n",
        "As an underlying model, we can use the binomial distribution which is a discrete probability distribution for the number of successes in a sequence of n independent binary experiments (e.g., coin flips resulting in \"heads\" or \"tails\"). The binomial distribution is conditioned on the parameters  n  (number of trials) and  p  the probability of flipping \"heads\". The probability mass function of the binomial distribution, which determines the probability of observing  k  \"heads\" with parameters p and n, is defined as:\n",
        "\n",
        "$$ f(k|p,n) = \\binom nk p^k (1-p)^{n-k} $$\n",
        "\n",
        "We can rewrite the above Bayes rules as:\n",
        "\n",
        "$$ P(p|k,n) \\propto P(k|p,n) \\times P(p) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsM_We8TWWb9"
      },
      "source": [
        "In case of the Binomial distribution as likelihood, we can use the Beta distribution as a conjugate prior. This means that also the posterior distribution will be of the same family (i.e., Beta distribution) as the prior distribution.\n",
        "\n",
        "The beta distribution is a continuous probability distribution over the interval  [0,1] . The PDF of the Beta distribution is defined as:\n",
        "\n",
        "$$ f(x|\\alpha,\\beta) =  \\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} =  \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, x^{\\alpha-1}(1-x)^{\\beta-1} $$\n",
        "\n",
        "$B$  is the beta function and is a normalization constant to ensure that the probability integrates to 1. $Γ$ is the gamma function.\n",
        "\n",
        "$α$ and $β$ are positive shape parameters controlling the shape of the distribution. If they are >=1, we can think of them as pseudo counts; i.e., counts before observing the data.\n",
        "\n",
        "So for our example, α would specify the pseudo counts of observing a \"heads\" flip, while β would refer to the counts of observing a \"tails\" flip. \n",
        "\n",
        "Let's visualize some examples next for the case of $α$>=1 and $β$>=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVDYd6A2WWb_"
      },
      "source": [
        "# helper function for plotting\n",
        "def plot_beta(a,b,ax, print_interval=True):\n",
        "    ax.set_xlabel(\"p\")\n",
        "    ax.set_ylabel(\"probability density\")\n",
        "    x = np.linspace(0.00,1, 100)\n",
        "    label = \"$\\\\alpha= \" + str(a) + \", \\\\beta=\" + str(b) + \"$\"\n",
        "    dist = beta(a,b)                                            # Beta distribution as prior having parameters alpha=a and beta=b\n",
        "    # plot density\n",
        "    ax.plot(x, dist.pdf(x), lw=2, alpha=0.6, label=label)       # visualize the prior for possible values of probability\n",
        "    # determine the 95% highest density interval\n",
        "    if print_interval:\n",
        "        print(\"Interval containing 95% of the distribution: \", dist.interval(0.95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDZpCN45WWcB"
      },
      "source": [
        "Let us consider a symmetric Beta distribution. This means that we want to express our prior belief that the coin is fair. We can express this prior with the shape parameters  α  and  β  which we can interpret as pseudo counts. As we have a fair belief, we set both shape parameters to the same value. Let us start with  α=β=10 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keNt9z2-WWcC"
      },
      "source": [
        "fig, ax = plt.subplots(1,1)\n",
        "plot_beta(10,10,ax)                                        # visualize prior for alpha= no. of heads= 10, beta= no. of tails= 10\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg_064N6WWcE"
      },
      "source": [
        "It can be seen that the parameter  p=0.5  receives the highest density. However, as we keep our pseudo counts relatively low, we also allow other parameter configurations to receive a certain density. If we determine the interval containing 95% of the distribution, we receive  0.29  as the lower and  0.71  as the upper boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4RjyjNjWWcF"
      },
      "source": [
        "fig, ax = plt.subplots(1,1)\n",
        "plot_beta(100,100,ax)                                      # visualize prior for alpha= no. of heads= 100, beta= no. of tails= 100\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuvrekNaWWcH"
      },
      "source": [
        "If we increase our symmetric shape parameters to a value of  100, we can see a much higher density for parameters around the fair probability of  0.5. The 95% highest density interval now lies between  0.43  and  0.71."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdgfILuyWWcI"
      },
      "source": [
        "### Maximum Likelihood Estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyAAUtwKWWcK"
      },
      "source": [
        "Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximize the likelihood that the process described by the model produced the data that was actually observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyD5zAo9WWcL"
      },
      "source": [
        "**Exercise 3:** Let’s suppose we have observed 20 data points from the normal distribution process. Recall that the normal distribution has 2 parameters. The mean, μ, and the standard deviation, σ. Different values of these parameters result in different curves. We want to know which curve was most likely responsible for creating the data points that we observed?.\n",
        "\n",
        "Maximum likelihood estimation is a method that will find the values of μ and σ that result in the curve that best fits the data. The values that we find are called the maximum likelihood estimates (MLE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCzKLzFRWWcM"
      },
      "source": [
        "# Create 20 data points from normal distribution\n",
        "mu = 5           # actual mean\n",
        "sd = 2.5         # actual standard deviation\n",
        "data = stats.norm.rvs(mu, sd, 20)          # generate 20 data points from normal distribution\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYOoz4gaWWcO"
      },
      "source": [
        "# Plot the density curve\n",
        "sns.distplot(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7inWhrjWWcP"
      },
      "source": [
        "We want to calculate the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we’ll make our first assumption. The assumption is that each data point is generated independently of the others. This assumption makes the maths much easier. If the events (i.e. the process that generates the data) are independent, then the total probability of observing all of the data is the product of observing each data point individually (i.e. the product of the marginal probabilities).\n",
        "\n",
        "The probability density of observing a single data point x, that is generated from a Gaussian(or Normal) distribution is given by:\n",
        "\n",
        "$$ P(x;μ,σ) =  \\frac{1}{σ\\sqrt{2π}}e^{\\frac{-(x-μ)^2}{2σ^2}} $$\n",
        "\n",
        "The total (joint) probability density of observing all data points is given by:\n",
        "\n",
        "$$ P(x_1, x_2,...;μ,σ) =  \\frac{1}{σ\\sqrt{2π}}  e^{\\frac{-(x_1-μ)^2}{2σ^2}} \\times  \\frac{1}{σ\\sqrt{2π}}  e^{\\frac{-(x_2-μ)^2}{2σ^2}} \\times  ...  \\times  \\frac{1}{σ\\sqrt{2π}}  e^{\\frac{-(x_n-μ)^2}{2σ^2}} $$\n",
        "\n",
        "We need to find out the values of μ and σ that result in giving the maximum value of the above expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzAy0U4rWWcR"
      },
      "source": [
        "The above expression for the total probability is hard to differentiate, so it is simplified by taking the natural logarithm of the expression. The natural logarithm is a monotonically increasing function. This means that if the value on the x-axis increases, the value on the y-axis also increases. It ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we work with the simpler log-likelihood instead of the original likelihood.\n",
        "\n",
        "Taking logs of the original expression gives us:\n",
        "\n",
        "$$ ln(P(x;μ,σ)) =  ln\\frac{1}{σ\\sqrt{2π}} - \\frac{(x_1-μ)^2}{2σ^2} + ln\\frac{1}{σ\\sqrt{2π}} - \\frac{(x_2-μ)^2}{2σ^2} \\ +  ... + ln\\frac{1}{σ\\sqrt{2π}} - \\frac{(x_n-μ)^2}{2σ^2} $$\n",
        "\n",
        "Using the laws of logarithms:\n",
        "\n",
        "$$ ln(P(x;μ,σ)) =  -n.ln(σ) - \\frac{n}{2}ln(2\\pi) - \\frac{1}{2σ^2}[(x_1-μ)^2 + (x_2-μ)^2 + ... + (x_n-μ)^2] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o74JBTXUWWcU"
      },
      "source": [
        "To find the MLE of the mean(μ), take the partial derivative of the function with respect to μ, giving:\n",
        "\n",
        "$$ \\frac{d\\ ln(P(x;μ,σ))}{dμ} =  \\frac{1}{σ^2}[-n.μ + x_1 + x_2 + ... + x_n] $$\n",
        "\n",
        "Finally, setting the left hand side of the equation to zero and then rearranging for μ gives:\n",
        "\n",
        "$$ μ = \\frac{\\sum{x_i}}{n} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8I8mIsyWWcW"
      },
      "source": [
        "# Calculate maximum likelihood estimate for μ\n",
        "mu_mle = sum(data)/len(data)\n",
        "mu_mle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmzRqhD8WWcX"
      },
      "source": [
        "sns.distplot(data)\n",
        "plt.plot([mu,mu], [0,0.15], 'red', label= 'Actual mean')            # plot the actual mean\n",
        "plt.plot([mu_mle,mu_mle], [0,0.15], 'green', label= 'MLE mean')     # plot the estimated mean\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZEQBYoWWcZ"
      },
      "source": [
        "Similarly, to find the MLE of the standard deviation(σ), take the partial derivative of the function with respect to σ, giving:\n",
        "\n",
        "$$ \\frac{d\\ ln(P(x;μ,σ))}{dσ} =  -\\frac{n}{σ}  + \\frac{1}{σ^3}[(x_1-μ)^2 + (x_2-μ)^2 + ... + (x_n-μ)^2] $$\n",
        "\n",
        "Finally, setting the left hand side of the equation to zero and then rearranging for σ gives:\n",
        "\n",
        "$$ σ = \\sqrt{\\frac{\\sum_1^n{(x_i-μ)^2}}{n}} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og5LHuu5WWca"
      },
      "source": [
        "# Calculate the maximum likelihood estimate for σ\n",
        "sd_mle = np.sqrt(sum((data-mu_mle)**2) / len(data))\n",
        "\n",
        "print('Actual SD: {:.3}'.format(sd))          # display the actual standard deviation\n",
        "print('MLE SD: {:.3}'.format(sd_mle))         # display the estimated standard deviation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRVNs5e1WWcb"
      },
      "source": [
        "### Conjugate Prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7REyWUAWWcb"
      },
      "source": [
        "In Bayesian probability theory, if the posterior distributions $p(θ | x)$ are in the same probability distribution family as the prior probability distribution $p(θ)$, the prior and posterior are then called conjugate distributions, and the prior is called a *conjugate prior* for the likelihood function $p(x | θ)$. \n",
        "\n",
        "For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian. This means that the Gaussian distribution is a conjugate prior for the likelihood that is also Gaussian. \n",
        "\n",
        "The concept, as well as the term \"conjugate prior\", was introduced by Howard Raiffa and Robert Schlaifer in their work on Bayesian decision theory. A similar concept had been discovered independently by George Alfred Barnard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKkM00snWWcc"
      },
      "source": [
        "Consider the general problem of inferring a (continuous) distribution for a parameter $θ$ given some data $x$. From Bayes' theorem, the posterior distribution is equal to the product of the likelihood function ${\\displaystyle \\theta \\mapsto p(x\\mid \\theta )\\!}$ and prior ${\\displaystyle p(\\theta )\\!}$, normalized (divided) by the probability of the data ${\\displaystyle p(x)\\!}$:\n",
        "\n",
        "$$ p(θ\\ |\\ x) = \\frac{p(x\\ |\\ θ).p(θ)}{p(x)} $$\n",
        "\n",
        "$$ \\   \\   \\    = \\frac{p(x\\ |\\ θ).p(θ)}{∫_{θ'}p(x,θ')dθ'} $$\n",
        "\n",
        "$$ \\   \\   \\    = \\frac{p(x\\ |\\ θ).p(θ)}{∫_{θ'}p(x\\ |\\ θ')p(θ')dθ'} $$\n",
        "\n",
        "Note: denominator not a function of θ ⇒ just normalizing term\n",
        "\n",
        "$$ \\underbrace{p(θ)}_{parametric}\\ \\   \\mapsto \\ \\  \\underbrace{p(x\\ |\\ θ)}_{parametric}p(θ) \\ \\  \\mapsto \\ \\ P(θ\\ |\\ x) \\propto \\underbrace{p(x\\ |\\ θ)p(θ)}_{mess?} $$\n",
        "\n",
        "Conjugacy: require $p(θ)$ and $p(\\ θ |\\ x)$ to be of the same form. For example, for binomial likelihood, the beta prior becomes a beta posterior.\n",
        "\n",
        "$$ \\underbrace{p(θ)}_{beta}\\ \\   \\mapsto \\ \\  \\underbrace{p(x\\ |\\ θ)}_{binomial}p(θ) \\ \\  \\mapsto \\ \\ \\underbrace{P(θ\\ |\\ x)}_{beta} $$\n",
        "\n",
        "$p(θ)$ and $p(θ\\ |\\ x)$ are then called conjugate distributions.\n",
        "\n",
        "Some of the conjugate priors are shown in the table below\n",
        "\n",
        "|  Likelihood  |  Model parameters  |   Conjugate Prior   |   Posterior |\n",
        "|:--------------|:-----------|:------------|:------------|\n",
        "|  Bernoulli  | *p* (probability) |  Beta    |   Beta      |\n",
        "|  Binomial  | *p* (probability) |  Beta    |   Beta      |\n",
        "|  Negative Binomial  | *p* (probability) |  Beta    |   Beta      |\n",
        "|  Poisson  | *λ* (rate) |  Gamma    |   Gamma      |\n",
        "|  Geometric  | *p* (probability) |  Beta    |   Beta      |\n",
        "|  Exponential  | *λ* (rate) |  Gamma    |   Gamma      |\n",
        "|  Normal (with known variance)  | *μ* (mean) |  Normal    |   Normal      |\n",
        "|  Normal (with known mean)  | *$σ^2$* (variance) |  Inverse Gamma    |   Inverse Gamma      |\n",
        "|  Multinomial  | *p* (probability vector) |  Dirichlet    |   Dirichlet      |\n",
        "\n",
        "\n",
        "To know more about other conjugate priors click [here](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title 1. Which of the following is meaningful only if one takes a Bayesian view of probability?  { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\",\"Probabilistic curve-fitting with a gaussian noise model\",\"Probabilistic curve-fitting with the model parameters treated as random variables\", \"Probabilistic curve-fitting to obtain a predictive distribution for test data\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}