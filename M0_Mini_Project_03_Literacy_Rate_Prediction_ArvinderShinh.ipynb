{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M0_Mini_Project_03_Literacy_Rate_Prediction_ArvinderShinh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlakireddy-cds/sample/blob/main/M0_Mini_Project_03_Literacy_Rate_Prediction_ArvinderShinh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWR22OAlTBlU"
      },
      "source": [
        "# Advanced Certification in Computational Data Science\n",
        "## A Program by IISc and TalentSprint\n",
        "### Mini-Project (Ungraded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIfh2REWFr6p"
      },
      "source": [
        "## Learning Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrIlNFZeKMsx"
      },
      "source": [
        "At the end of this experiment, you will be able to:\n",
        "\n",
        "* perform Data preprocessing\n",
        "* implement ML classification algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vgcWwOF2cK"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfv616s92gl_"
      },
      "source": [
        "We will be using district wise demographics, enrollments, and teacher indicator data to predict whether the literacy rate is high/ medium/ low in each district."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md2IjdMdGCWm"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5ztQVbKMsz"
      },
      "source": [
        "Data preprocessing is an important step in solving every machine learning problem. Most of\n",
        "the datasets used with Machine Learning problems need to be processed / cleaned / transformed\n",
        "so that a Machine Learning algorithm can be trained on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsxaJLZAKMs0"
      },
      "source": [
        "There are different steps involved in Data Preprocessing. These steps are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF3Eg-5pKMs1"
      },
      "source": [
        "    1. Data Cleaning → In this step the primary focus is on\n",
        "        - Handling missing data\n",
        "        - Handling noisy data\n",
        "        - Detection and removal of outliers\n",
        "    \n",
        "    2. Data Integration → This process is used when data is gathered from various data sources\n",
        "    and data are combined to form consistent data. This data after performing cleaning is used\n",
        "    for analysis.\n",
        "    \n",
        "    3. Data Transformation → In this step we will convert the raw data into a specified format according to the need of the model we are building. There are many options used for\n",
        "    transforming the data as below:\n",
        "        - Normalization\n",
        "        - Aggregation\n",
        "        - Generalization\n",
        "        \n",
        "    4. Data Reduction → Following data transformation and scaling, the redundancy within the data is removed and is organized efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4UpjelxSBoGN"
      },
      "source": [
        "# @title Download the datasets\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/aiml/Experiment_related_data/B15_Data_Munging.zip\")\n",
        "    ipython.magic(\"sx unzip B15_Data_Munging.zip\")\n",
        "    print(\"Data downloaded successfully\")\n",
        "    return\n",
        "\n",
        "setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7VD8dJgGhVw"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZSlj_nWKMs4"
      },
      "source": [
        "## Exercise 1 - Load and Explore the Data \n",
        "1. We have three different files\n",
        "\n",
        "  * Districtwise_Basicdata.csv\n",
        "  * Districtwise_Enrollment_details_indicator.csv\n",
        "  * Districtwise_Teacher_indicator.csv\n",
        "\n",
        "  These files contain the necessary data to solve the problem. <br>\n",
        "\n",
        "2. Load the files based on **team allocation** mentioned below. Observe the header level details, data records while loading the data.\n",
        "  \n",
        "  Hint : Use read_csv from pandas with [skiprows or header](https://towardsdatascience.com/import-csv-files-as-pandas-dataframe-with-skiprows-skipfooter-usecols-index-col-and-header-fbf67a2f92a) options.\n",
        "\n",
        "3. Read the columns of the dataset and rename if required.\n",
        "\n",
        "  Hint : Rename column names (if any) using the following [link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7xw5qs68uWE"
      },
      "source": [
        "Team allocation for dataset selection\n",
        "\n",
        "    Team A = 1,3,5,7,9\n",
        "        Districtwise_Basicdata.csv\n",
        "        Districtwise_Enrollment_details_indicator.csv\n",
        "\n",
        "    Team B = 2,4,6,8,10\n",
        "        Districtwise_Basicdata.csv\n",
        "        Districtwise_Teacher_indicator.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XRregSb9wdB"
      },
      "source": [
        "# Importing all the required packages and add neccesary imports if required\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-GbwZKBD0L"
      },
      "source": [
        "# YOUR CODE HERE for loading and exploring the datasets\n",
        "Districtwise_Basicdata = pd.read_csv('Districtwise_Basicdata.csv', skiprows = 1)\n",
        "Districtwise_Teacher_indicator = pd.read_csv('Districtwise_Teacher_indicator.csv', skiprows = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6uStBkAzlSs",
        "outputId": "1c14e23c-a0cc-4695-ee9c-a8ceb19f8c2e"
      },
      "source": [
        "Districtwise_Basicdata.rename(columns=str.lower, inplace=True)\n",
        "Districtwise_Teacher_indicator.rename(columns={\"ac_year\": \"year\"}, inplace=True)\n",
        "\n",
        "print(Districtwise_Basicdata.shape)\n",
        "print(Districtwise_Teacher_indicator.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1324, 19)\n",
            "(1324, 181)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2A3OKraKMs9"
      },
      "source": [
        "## Exercise 2  - Data Integration\n",
        "\n",
        "As the required data is present in different datasets, we need to **integrate both to make a single dataframe/dataset**.\n",
        "  * For integrating the datasets, create a unique identifier for each row in both the dataframes so that it can be used to map the data in different files.\n",
        "   \n",
        "    * Combine year, state code, district code columns and form a new unique identifier column, refer this [link](https://stackoverflow.com/questions/33098383/merge-multiple-column-values-into-one-column-in-python-pandas).\n",
        "    * Set the identifier column as the index for each dataframe.\n",
        "\n",
        "    * Integrate the dataframes using the above index\n",
        "     \n",
        "     Hint: For merging or joining the datasets, refer to this [link](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
        "\n",
        "**Example:** Data of the district Anantapur in Andrapradesh, which is present in different files should form a single row after integrating the datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXvBjPIH4CWH"
      },
      "source": [
        "df = pd.merge(Districtwise_Basicdata, Districtwise_Teacher_indicator, on=[\"year\",\"statecd\",\"distcd\"], how=\"outer\", validate=\"one_to_one\", indicator=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcX4aRsKMtE"
      },
      "source": [
        "## Exercise 3 - Data Cleaning \n",
        "\n",
        "1.  **overall_lit** is our target variable. Delete rows with missing overall_lit value\n",
        "\n",
        "   Hint: Refer to the link [dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
        "\n",
        "\n",
        "2.  Convert categorical values to numerical values.\n",
        "\n",
        "  For example, if a feature contains categorical values such as dog, cat, mouse, etc then replace them with 1, 2, 3, etc or use [Sklearn LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) \n",
        "\n",
        "3. Replace the missing values in any other column appropriately with mean / median / mode.\n",
        "\n",
        "  Hint: Use pandas [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) function to replace the missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhKVYJ24xVn",
        "outputId": "3b9d0e3e-72b1-4527-8740-981c6df2d9a3"
      },
      "source": [
        "df = df[df[\"overall_lit\"].notna()]\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1268, 198)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koy8KUP2zlSv"
      },
      "source": [
        "labelEncoder = {'High': 1, 'Medium': 2, 'Low': 3}\n",
        "df[\"overall_lit\"] = df[\"overall_lit\"].map(labelEncoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dY_QI3PzlSw"
      },
      "source": [
        "df.fillna(df.mean(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzsY-knQKMtI"
      },
      "source": [
        "## Exercise 4 \n",
        "\n",
        "1. Remove the unneccesary columns which are not contributing to the overall literacy rate\n",
        "\n",
        "2. Verify if there are any duplicate columns and remove them.\n",
        "\n",
        "  For example: state name and district name are same as state code and district code.\n",
        "\n",
        "3. Make sure that the final dataframe has no null or nan values. Delete the rows with missing values.\n",
        "\n",
        "   Hint: Verify with df.isna() for nan values in the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Azol6_zlSx"
      },
      "source": [
        "df_unique_cnt = df.apply(lambda s: s.drop_duplicates().count())\n",
        "df_unique_cnt[df_unique_cnt == 1]\n",
        "\n",
        "df.drop(columns=['tch_nr_p6'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hU2poUeKMtT"
      },
      "source": [
        "df.drop(columns=['statename_x', 'statename_y', 'distname_x', 'distname_y', '_merge'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-xh0kYVzlSy",
        "outputId": "da20a30d-806f-4ba1-9162-434e196b7ad0"
      },
      "source": [
        "df.isna().any().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzXGJNFyzlSz"
      },
      "source": [
        "df.set_index([\"year\",\"statecd\",\"distcd\"], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJo0t2403sf7"
      },
      "source": [
        "## Exercise 5 - Apply Correlation Matrix\n",
        "\n",
        "Correlation is a statistical technique that can show whether, and how strongly, pairs of variables are related. More number of features does not imply better accuracy. More features may lead to a decline in the accuracy and create noise in the model, if they contain any irrelevant features.\n",
        "\n",
        "*Features with high correlation value will imply the same meaning. Hence remove the highly correlated features*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRn2Zv-pwdR"
      },
      "source": [
        "**Function Description:**\n",
        "\n",
        "Create a function `remove_Highly_Correlated()` function, which removes highly correlated features in the dataframe.\n",
        "- Creates a correlation matrix of row and column wise features\n",
        "- Extracts only upper triangular matrix as correlation matrix, which will have the same values below and above the diagonal\n",
        "- Removes columns which are having correlation value more than the threshold value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEN36Fj0o2TQ"
      },
      "source": [
        "def remove_Highly_Correlated(df, bar=0.9):\n",
        "  # Creates correlation matrix\n",
        "  corr = df.corr()\n",
        "  \n",
        "  print(corr)\n",
        "  # Set Up Mask To Hide Upper Triangle\n",
        "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "  tri_df = corr.mask(mask)\n",
        "\n",
        "  # Finding features with correlation value more than specified threshold value (bar=0.9)\n",
        "  highly_cor_col = [col for col in tri_df.columns if any(tri_df[col] > bar )]\n",
        "  print(\"length of highly correlated columns\",len(highly_cor_col))\n",
        "    \n",
        "  print(tri_df)\n",
        "\n",
        "  # Drop the highly correlated columns\n",
        "  reduced_df = df.drop(highly_cor_col, axis = 1)\n",
        "  print(\"shape of total data\",df.shape,\"shape of reduced data\",reduced_df.shape)\n",
        "  return reduced_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMZGXL5O45",
        "outputId": "756f9dfe-f3ba-4866-a844-ee1ea563adf6"
      },
      "source": [
        "df_Correlated = df.drop(columns=['overall_lit'])\n",
        "df_reduced = remove_Highly_Correlated(df_Correlated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  blocks  clusters  villages  totschools  p_06_pop  p_urb_pop  \\\n",
            "blocks          1.000000  0.719836  0.480601    0.647941  0.501250   0.017008   \n",
            "clusters        0.719836  1.000000  0.595567    0.757229  0.541815  -0.029617   \n",
            "villages        0.480601  0.595567  1.000000    0.829773  0.615654  -0.182677   \n",
            "totschools      0.647941  0.757229  0.829773    1.000000  0.719811  -0.017408   \n",
            "p_06_pop        0.501250  0.541815  0.615654    0.719811  1.000000   0.108293   \n",
            "...                  ...       ...       ...         ...       ...        ...   \n",
            "trn_tch_f7      0.341426  0.257034  0.122937    0.217627  0.161162   0.139115   \n",
            "prof_trn_tch_r  0.541519  0.673048  0.374279    0.627701  0.593879   0.341628   \n",
            "prof_trn_tch_p  0.248884  0.222227  0.342820    0.286879  0.247422  -0.004178   \n",
            "days_nontch    -0.114235 -0.035032  0.003515    0.003734  0.008506  -0.050434   \n",
            "tch_nontch      0.259996  0.201848  0.332028    0.362174  0.294288   0.017335   \n",
            "\n",
            "                sexratio  sexratio_06  growthrate  p_sc_pop  ...  trn_tch_f2  \\\n",
            "blocks          0.161036     0.085731   -0.083531  0.204320  ...    0.084100   \n",
            "clusters        0.170149    -0.033848   -0.047465  0.219739  ...    0.244960   \n",
            "villages        0.070475     0.005409    0.050561  0.308318  ...    0.035163   \n",
            "totschools      0.060747    -0.051312    0.054206  0.291291  ...    0.057268   \n",
            "p_06_pop       -0.088315    -0.094779    0.204884  0.228444  ...    0.224934   \n",
            "...                  ...          ...         ...       ...  ...         ...   \n",
            "trn_tch_f7      0.070233    -0.100324   -0.173196  0.130867  ...    0.108842   \n",
            "prof_trn_tch_r  0.102361    -0.210454   -0.080987  0.230981  ...    0.415080   \n",
            "prof_trn_tch_p -0.000744    -0.036376    0.061614  0.149532  ...   -0.017871   \n",
            "days_nontch    -0.036050     0.015889    0.074378  0.003559  ...   -0.191204   \n",
            "tch_nontch      0.013975     0.078123   -0.024365  0.110399  ...   -0.007534   \n",
            "\n",
            "                trn_tch_f3  trn_tch_f4  trn_tch_f5  trn_tch_f6  trn_tch_f7  \\\n",
            "blocks           -0.021419   -0.044663    0.314842   -0.003779    0.341426   \n",
            "clusters         -0.067362   -0.001766    0.179766    0.019829    0.257034   \n",
            "villages         -0.111963    0.128217    0.015043   -0.024558    0.122937   \n",
            "totschools       -0.079073    0.149541    0.126800   -0.044931    0.217627   \n",
            "p_06_pop          0.083213    0.083656    0.133256    0.037790    0.161162   \n",
            "...                    ...         ...         ...         ...         ...   \n",
            "trn_tch_f7        0.057910    0.028482    0.238197    0.217337    1.000000   \n",
            "prof_trn_tch_r    0.195742   -0.006906    0.484341    0.139404    0.434521   \n",
            "prof_trn_tch_p   -0.054100    0.018719    0.021796   -0.002195    0.024555   \n",
            "days_nontch      -0.024848    0.013045   -0.132329   -0.068549   -0.086794   \n",
            "tch_nontch        0.107783    0.115688    0.330571   -0.003512    0.222979   \n",
            "\n",
            "                prof_trn_tch_r  prof_trn_tch_p  days_nontch  tch_nontch  \n",
            "blocks                0.541519        0.248884    -0.114235    0.259996  \n",
            "clusters              0.673048        0.222227    -0.035032    0.201848  \n",
            "villages              0.374279        0.342820     0.003515    0.332028  \n",
            "totschools            0.627701        0.286879     0.003734    0.362174  \n",
            "p_06_pop              0.593879        0.247422     0.008506    0.294288  \n",
            "...                        ...             ...          ...         ...  \n",
            "trn_tch_f7            0.434521        0.024555    -0.086794    0.222979  \n",
            "prof_trn_tch_r        1.000000        0.116407    -0.076093    0.239105  \n",
            "prof_trn_tch_p        0.116407        1.000000    -0.076434    0.022613  \n",
            "days_nontch          -0.076093       -0.076434     1.000000   -0.038637  \n",
            "tch_nontch            0.239105        0.022613    -0.038637    1.000000  \n",
            "\n",
            "[163 rows x 163 columns]\n",
            "length of highly correlated columns 0\n",
            "                  blocks  clusters  villages  totschools  p_06_pop  p_urb_pop  \\\n",
            "blocks               NaN       NaN       NaN         NaN       NaN        NaN   \n",
            "clusters        0.719836       NaN       NaN         NaN       NaN        NaN   \n",
            "villages        0.480601  0.595567       NaN         NaN       NaN        NaN   \n",
            "totschools      0.647941  0.757229  0.829773         NaN       NaN        NaN   \n",
            "p_06_pop        0.501250  0.541815  0.615654    0.719811       NaN        NaN   \n",
            "...                  ...       ...       ...         ...       ...        ...   \n",
            "trn_tch_f7      0.341426  0.257034  0.122937    0.217627  0.161162   0.139115   \n",
            "prof_trn_tch_r  0.541519  0.673048  0.374279    0.627701  0.593879   0.341628   \n",
            "prof_trn_tch_p  0.248884  0.222227  0.342820    0.286879  0.247422  -0.004178   \n",
            "days_nontch    -0.114235 -0.035032  0.003515    0.003734  0.008506  -0.050434   \n",
            "tch_nontch      0.259996  0.201848  0.332028    0.362174  0.294288   0.017335   \n",
            "\n",
            "                sexratio  sexratio_06  growthrate  p_sc_pop  ...  trn_tch_f2  \\\n",
            "blocks               NaN          NaN         NaN       NaN  ...         NaN   \n",
            "clusters             NaN          NaN         NaN       NaN  ...         NaN   \n",
            "villages             NaN          NaN         NaN       NaN  ...         NaN   \n",
            "totschools           NaN          NaN         NaN       NaN  ...         NaN   \n",
            "p_06_pop             NaN          NaN         NaN       NaN  ...         NaN   \n",
            "...                  ...          ...         ...       ...  ...         ...   \n",
            "trn_tch_f7      0.070233    -0.100324   -0.173196  0.130867  ...    0.108842   \n",
            "prof_trn_tch_r  0.102361    -0.210454   -0.080987  0.230981  ...    0.415080   \n",
            "prof_trn_tch_p -0.000744    -0.036376    0.061614  0.149532  ...   -0.017871   \n",
            "days_nontch    -0.036050     0.015889    0.074378  0.003559  ...   -0.191204   \n",
            "tch_nontch      0.013975     0.078123   -0.024365  0.110399  ...   -0.007534   \n",
            "\n",
            "                trn_tch_f3  trn_tch_f4  trn_tch_f5  trn_tch_f6  trn_tch_f7  \\\n",
            "blocks                 NaN         NaN         NaN         NaN         NaN   \n",
            "clusters               NaN         NaN         NaN         NaN         NaN   \n",
            "villages               NaN         NaN         NaN         NaN         NaN   \n",
            "totschools             NaN         NaN         NaN         NaN         NaN   \n",
            "p_06_pop               NaN         NaN         NaN         NaN         NaN   \n",
            "...                    ...         ...         ...         ...         ...   \n",
            "trn_tch_f7        0.057910    0.028482    0.238197    0.217337         NaN   \n",
            "prof_trn_tch_r    0.195742   -0.006906    0.484341    0.139404    0.434521   \n",
            "prof_trn_tch_p   -0.054100    0.018719    0.021796   -0.002195    0.024555   \n",
            "days_nontch      -0.024848    0.013045   -0.132329   -0.068549   -0.086794   \n",
            "tch_nontch        0.107783    0.115688    0.330571   -0.003512    0.222979   \n",
            "\n",
            "                prof_trn_tch_r  prof_trn_tch_p  days_nontch  tch_nontch  \n",
            "blocks                     NaN             NaN          NaN         NaN  \n",
            "clusters                   NaN             NaN          NaN         NaN  \n",
            "villages                   NaN             NaN          NaN         NaN  \n",
            "totschools                 NaN             NaN          NaN         NaN  \n",
            "p_06_pop                   NaN             NaN          NaN         NaN  \n",
            "...                        ...             ...          ...         ...  \n",
            "trn_tch_f7                 NaN             NaN          NaN         NaN  \n",
            "prof_trn_tch_r             NaN             NaN          NaN         NaN  \n",
            "prof_trn_tch_p        0.116407             NaN          NaN         NaN  \n",
            "days_nontch          -0.076093       -0.076434          NaN         NaN  \n",
            "tch_nontch            0.239105        0.022613    -0.038637         NaN  \n",
            "\n",
            "[163 rows x 163 columns]\n",
            "shape of total data (1268, 163) shape of reduced data (1268, 163)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_yZN3u9Vud7"
      },
      "source": [
        "## Exercise 6\n",
        "\n",
        "Perform Mean Correction and Standard Scaling on the data feature/column wise.\n",
        "\n",
        "**Hint:** In order to understand the idea behind the terms used above, you may refer the following link: \n",
        "\n",
        "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAiQVqsPl1ZM"
      },
      "source": [
        "df_mean = df_reduced.apply(\"mean\")\n",
        "df_std = df_reduced.apply(\"std\")\n",
        "\n",
        "df_std_scaling = (df_reduced-df_mean)/df_std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNaqZf-szlS3"
      },
      "source": [
        "df = pd.concat([df_std_scaling, df['overall_lit']], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTB9sMRsVxyj"
      },
      "source": [
        "## Exercise 7\n",
        "\n",
        "Apply different classifiers on the preprocessed data and figure out which classifier gives the best result.\n",
        "\n",
        "* Split the data into train and test\n",
        "\n",
        "* Fit the model with train data and find its accuracy on test data\n",
        "\n",
        "### Expected Accuracy is above 90%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRgkeFVBqv0W",
        "outputId": "2b1ea0f6-166b-47cb-935a-283192d1383c"
      },
      "source": [
        "features = df.iloc[:,:-1].values\n",
        "labels = df.iloc[:,-1].values\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1268, 163)\n",
            "(1268,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoOvV8tqzlS9"
      },
      "source": [
        "# Exporting the model into a dot file\n",
        "import os\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN1asZxkzlS9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state = 42, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_fjhvfLzlS-"
      },
      "source": [
        "Implement Decision Tree on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZqd0-GczlS-"
      },
      "source": [
        "tree_clf = tree.DecisionTreeClassifier(criterion='entropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N64Z1opzlS_"
      },
      "source": [
        "# Fitting the data\n",
        "tree_clf = tree_clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeNlCGMBzlS_"
      },
      "source": [
        "# Predict the labels for test data\n",
        "pred = tree_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1P30IrTzlTA",
        "outputId": "faadcd84-d113-4fea-9d4a-0cdbe6e7c1ab"
      },
      "source": [
        "# Calculating accuracy\n",
        "accuracy_score(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.937007874015748"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp2VzrRBzlTB"
      },
      "source": [
        "Implement KNN Classifier on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubuhFaBNzlTB",
        "outputId": "66a9103c-38b7-41e6-be79-db773500e0f3"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFVHYNT-zlTC"
      },
      "source": [
        "# Predict the labels for test data\n",
        "y_pred = neigh.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPkB6Qu6zlTC",
        "outputId": "a3e8045e-2068-4677-dcbf-d6080e46c399"
      },
      "source": [
        "# Calculating accuracy\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7519685039370079"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jt1ldkwzlTD"
      },
      "source": [
        "Implement Linear Classifier on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7OT0Ea_zlTD",
        "outputId": "008b57b6-b0cd-4faf-f2b5-2d719d7093a5"
      },
      "source": [
        "linear_clf = LogisticRegression(random_state=0)\n",
        "linear_clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Arvinder Shinh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CdoNwXozlTD"
      },
      "source": [
        "# Predict the labels for test data\n",
        "pred = linear_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIyINg5zzlTD",
        "outputId": "7bbffc26-c475-4ac6-8c52-3f148d909b82"
      },
      "source": [
        "# Calculating accuracy\n",
        "accuracy_score(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8779527559055118"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSI1tXXzlTE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}